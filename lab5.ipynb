{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidad del valle de Guatemala\n",
    "Andres Quinto\n",
    "Andree Toledo\n",
    "Laboratorio 5 DS / Analisis de sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import io \n",
    "import wordcloud as wc\n",
    "import random\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colocamos los valores en lowercase para que no haya problemas con las mayusculas\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df['text'].str.lower()\n",
    "df['location'].str.lower()\n",
    "df['keyword'].str.lower()\n",
    "#print(df.head())\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminacion de caracteres especiales y signos de puntuacion\n",
    "df.text = df.text.str.replace('[#,@,&,(,),!,?,/,{,},%,!]', '')\n",
    "df.location = df.location.str.replace('[#,@,&,(,),!,?,/,{,},%,!]', '')\n",
    "df.keyword = df.keyword.str.replace('[#,@,&,(,),!,?,/,{,},%,!]', '')\n",
    "#Eliminacion de url\n",
    "df.text = [re.sub('\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*','',i) for i in df.text]\n",
    "df.text = [re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', i) for i in df.text]\n",
    "#Eliminacion de emoticonos\n",
    "df.text = [re.sub('[^a-zA-Z0-9 ]+','', i) for i in df.text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminacion de conjunciones (stopwords)\n",
    "import io \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from cgitb import text\n",
    "\n",
    "#https://towardsdatascience.com/how-to-clean-text-data-639375414a2f \n",
    "\n",
    "stopwords = set(stopwords.words('english') + ['the', 'i', 'a', 'deeds', 'im','rt','jk','btw','lol','yolo','lmao','lmfao','fb','like','get','em', 'I', 'The', 'A', 'Amp', 'amp'])\n",
    "expresiones = ['im','rt','jk','btw','lol','yolo','lmao','lmfao','fb','like','get','em', 'Im', 'in', 'In', '2']\n",
    "for i in expresiones:\n",
    "    stopwords.add(i)\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "# Creation of the colum named 'text without stopwords'\n",
    "df['text_without_stopwords'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "# print(df.text)\n",
    "clean_tweets = df['text_without_stopwords']\n",
    "print(df['text_without_stopwords'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Obtencion de la frecuencia de las palabras tanto de los tweets de desastres como de los que no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frec_dic_tweets = {}\n",
    "for word in clean_tweets:\n",
    "    if word in frec_dic_tweets:\n",
    "        frec_dic_tweets[word] += 1\n",
    "    else:\n",
    "        frec_dic_tweets[word] = 1\n",
    "\n",
    "tweets_frecuency = pd.DataFrame.from_dict(frec_dic_tweets, orient='index')\n",
    "tweets_frecuency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que palabra se repite mas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creacion de una lista de palabras para cada tweet\n",
    "word_count = Counter()\n",
    "\n",
    "for title in clean_tweets:\n",
    "    word_count.update(word.strip('.,?!\"\\'').lower() for word in title.split())\n",
    "#A continuacion se crea una lista con los 10 tweets mas frecuentes\n",
    "word_count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_tweet = WordCloud(background_color=\"white\",width=1000,height=1000, max_words=10,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(word_count)\n",
    "plt.imshow(cloud_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Grafico de barras para las palabras mas repetidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_tweet = word_count.most_common(10)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "x_axis = [i[0] for i in datos_tweet]\n",
    "y_axis = [i[1] for i in datos_tweet]\n",
    "ax.bar(x_axis,y_axis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Grama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ph2aFDaurLEQ",
   "metadata": {
    "id": "ph2aFDaurLEQ"
   },
   "outputs": [],
   "source": [
    "global_data = df['text_without_stopwords']\n",
    "x = len(global_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val=round(x*0.1,0)\n",
    "random_sample=random.sample(list(global_data),int(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FsTi2sOEr9bS",
   "metadata": {
    "id": "FsTi2sOEr9bS"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lDo8hXaMufar",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDo8hXaMufar",
    "outputId": "4e55ea01-f925-4ae4-9c4c-8bff85eeb7f6"
   },
   "outputs": [],
   "source": [
    "digrama=[]\n",
    "print(digrama)\n",
    "size=2\n",
    "def ngram(ngrama, size):\n",
    "  for word in range(len(random_sample)):\n",
    "    try:\n",
    "      for item in ngrams(random_sample[word].split(),size):\n",
    "          ngrama.append(item)\n",
    "    except:\n",
    "        return\n",
    "\n",
    "ngram(digrama, size)\n",
    "print(digrama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5EDWw8HVmCDl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EDWw8HVmCDl",
    "outputId": "b99fc4e0-d866-40f8-fe21-7fd698e601cb"
   },
   "outputs": [],
   "source": [
    "trigrama=[]\n",
    "print(trigrama)\n",
    "size=3\n",
    "ngram(trigrama, size)\n",
    "print(trigrama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zcAZei1NL_ph",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcAZei1NL_ph",
    "outputId": "cdf5a121-1e2e-4816-f147-1d0337ccddea"
   },
   "outputs": [],
   "source": [
    "tetragrama=[]\n",
    "print(tetragrama)\n",
    "size=4\n",
    "ngram(tetragrama, size)\n",
    "print(tetragrama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D-YKCmEywhbX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-YKCmEywhbX",
    "outputId": "836031da-7837-4078-8715-12f8d3f1abf8"
   },
   "outputs": [],
   "source": [
    "pentgrama=[]\n",
    "print(pentgrama)\n",
    "size=5\n",
    "ngram(pentgrama, size)\n",
    "print(pentgrama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Referencia de como realizar un analisis de sentimientos\n",
    "#https://www.kaggle.com/rohithreddy/analyzing-sentiments-of-tweets-using-nltk\n",
    "#https://www.arsys.es/blog/analisis-sentimientos-python-jupyter-notebooks\n",
    "\n",
    "#Realizamos una funcion para tokenizar los tweets\n",
    "def get_tweets(texts):\n",
    "    corpus = []\n",
    "    for text in texts:\n",
    "        corpus.extend(word_tokenize(text))\n",
    "    return corpus\n",
    "\n",
    "# clasificamos los twees como desastres o no desastres\n",
    "disaster_tweet = get_tweets(df[df[\"target\"] == 1][\"text\"])\n",
    "non_disaster_tweet = get_tweets(df[df[\"target\"] == 0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "SX = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentimentValue = np.zeros(len(df['text']))\n",
    "for k in range(len(df['text'])):\n",
    "    sentimentValue[k] = SX.polarity_scores(df['text'][k]).get('compound')\n",
    "df.insert(loc=len(df.columns), column = 'sentimentValue', value = sentimentValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# media del dataset\n",
    "# sentimentValue es -0.15. \n",
    "print(\"Media de sentimiento: \", df['sentimentValue'].mean())\n",
    "\n",
    "# Los 10 tweets con mayor sentimiento positivo\n",
    "print(df.sort_values(by='sentimentValue').head(10)['text'])\n",
    "\n",
    "# Los 10 tweets con menor sentimiento negativo\n",
    "print(df.sort_values(by='sentimentValue', ascending=False).head(10)['text'])\n",
    "\n",
    "# Promedio de sentimentValue para tweets relacionados a desastre y no desastre.\n",
    "# La media de los tweets sobre desastre es -0.27.\n",
    "print(\"Media sentimentValue desastre: \", df[df[\"target\"] == 1]['sentimentValue'].mean())\n",
    "# La media de los tweets sobre no desastre es -0.06. \n",
    "print(\"Media sentimentValue no desastre: \", df[df[\"target\"] == 0]['sentimentValue'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76407a9a7fc59beee7a6b7311db6e2517dff2420fb9d212cfdbadd08521d1a27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
